{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq71asBBzvcw"
      },
      "outputs": [],
      "source": [
        "# üîß INSTALL EVERYTHING\n",
        "!pip install -q diffusers transformers accelerate safetensors gradio_client opencv-python groq\n",
        "\n",
        "# üß† IMPORTS\n",
        "import torch, json, requests, cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from gradio_client import Client, handle_file\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "from groq import Groq\n",
        "\n",
        "# üîë GROQ API (LLM)\n",
        "groq = Groq(api_key=\"API KEY\")  # Replace with your key\n",
        "llm = groq.chat.completions\n",
        "\n",
        "def parse_instruction(prompt):\n",
        "    instruction_prompt = f'''You are an instruction parser.\n",
        "\n",
        "Instruction: \"{prompt}\"\n",
        "\n",
        "Output JSON:\n",
        "{{\"action\": \"remove\" or \"replace\", \"object\": \"<object>\", \"target\": \"<new object>\" or null}}\n",
        "\n",
        "Examples:\n",
        "remove tiger ‚Üí {{\"action\":\"remove\",\"object\":\"tiger\",\"target\":null}}\n",
        "replace tiger with lion ‚Üí {{\"action\":\"replace\",\"object\":\"tiger\",\"target\":\"lion\"}}\n",
        "\n",
        "ONLY return JSON.'''\n",
        "\n",
        "    response = llm.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": instruction_prompt}],\n",
        "        temperature=0.0,\n",
        "        max_completion_tokens=256,\n",
        "        top_p=1,\n",
        "        stream=False\n",
        "    )\n",
        "    return json.loads(response.choices[0].message.content.strip())\n",
        "\n",
        "# üéØ Get mask from Gradio Space (evf-sam2)\n",
        "def get_mask_from_gradio(image_path, object_prompt):\n",
        "    client = Client(\"wondervictor/evf-sam2\")\n",
        "    result_path = client.predict(\n",
        "        image_np=handle_file(image_path),\n",
        "        prompt=object_prompt,\n",
        "        semantic_type=False,\n",
        "        api_name=\"/inference_image\"\n",
        "    )\n",
        "    # üí° Load directly from file path returned\n",
        "    vis_image = Image.open(result_path).convert(\"RGB\")\n",
        "    np_img = np.array(vis_image)\n",
        "\n",
        "    # Extract mask based on highlight color (blue hues)\n",
        "    hsv = cv2.cvtColor(np_img, cv2.COLOR_RGB2HSV)\n",
        "    lower = np.array([100, 50, 50])\n",
        "    upper = np.array([140, 255, 255])\n",
        "    mask = cv2.inRange(hsv, lower, upper)\n",
        "\n",
        "    return Image.fromarray(mask)\n",
        "\n",
        "def inpaint_with_mask(image, mask, prompt=\"background\"):\n",
        "    import gc, torch\n",
        "    from diffusers import StableDiffusionXLInpaintPipeline, StableDiffusionXLImg2ImgPipeline\n",
        "\n",
        "    image = image.resize((1024, 1024))\n",
        "    mask = mask.resize((1024, 1024)).convert(\"L\")\n",
        "\n",
        "    # üß† Load Inpainting Pipeline\n",
        "    pipe = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # üñåÔ∏è Inpaint\n",
        "    result = pipe(prompt=prompt, image=image, mask_image=mask).images[0]\n",
        "\n",
        "    # üßπ Unload Inpainting to free memory\n",
        "    del pipe\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # üé® Load Refiner AFTER unloading inpainting\n",
        "    refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # ‚ú® Refine image\n",
        "    refined = refiner(prompt=prompt, image=result).images[0]\n",
        "\n",
        "    return refined\n",
        "\n",
        "\n",
        "\n",
        "# üîÅ MAIN FUNCTION\n",
        "def run_prompt_edit(image_path, user_prompt):\n",
        "    original_image = Image.open(image_path).convert(\"RGB\")\n",
        "    instr = parse_instruction(user_prompt)\n",
        "    action, obj, tgt = instr[\"action\"], instr[\"object\"], instr.get(\"target\")\n",
        "\n",
        "    print(f\"üß† Parsed: action={action}, object={obj}, target={tgt}\")\n",
        "\n",
        "    mask = get_mask_from_gradio(image_path, obj)\n",
        "    inpaint_prompt = tgt if action == \"replace\" else \"background\"\n",
        "\n",
        "    return inpaint_with_mask(original_image, mask, inpaint_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # will prompt for your token"
      ],
      "metadata": {
        "id": "x2dWybWuz259"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# üñºÔ∏è Auto-detect latest image file\n",
        "def get_latest_image(folder=\"/content/\"):\n",
        "    images = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    if not images:\n",
        "        raise FileNotFoundError(\"No image files found in the specified folder.\")\n",
        "    latest_image = max(images, key=os.path.getmtime)  # Most recently modified\n",
        "    return latest_image\n",
        "\n",
        "# üé§ Ask user for the task\n",
        "user_prompt = input(\"Enter your edit instruction (e.g., 'replace tiger with lion'): \")\n",
        "\n",
        "# üèÉ Run pipeline\n",
        "latest_image_path = get_latest_image(\"/content/\")\n",
        "print(f\"üì∏ Using latest image: {latest_image_path}\")\n",
        "\n",
        "result = run_prompt_edit(latest_image_path, user_prompt)\n",
        "\n",
        "# üíæ Save & show result\n",
        "output_path = \"/content/edited_image.png\"\n",
        "result.save(output_path)\n",
        "print(f\"‚úÖ Edited image saved at: {output_path}\")\n",
        "result.show()"
      ],
      "metadata": {
        "id": "0Ky9fWAsz301"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}